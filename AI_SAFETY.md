
# AI Safety & Ethical Intelligence

Aura uses advanced LLM reasoning to interpret complex health signals. To ensure this is done safely, the system implements three layers of intelligence verification.

## 1. The Safety Guard (Pre-Inference)
Before data is sent to the LLM, a heuristic validator checks for:
- **Biological Contradictions**: e.g., High Energy reported with 2 hours of sleep.
- **Distress Signals**: e.g., Stress > 9 with Mood < 3 for multiple days.
If detected, the system shifts into **Stabilizing Mode**, replacing proactive coaching with grounding advice and recovery prompts.

## 2. Confidence Scoring
Every "Deep Insight" generated by the Neural Core includes a **Confidence Score (0-100%)**.
- **High Confidence (>85%)**: Based on strong correlation between 14+ days of data.
- **Medium Confidence (60-85%)**: Suggests a probable pattern but requires more validation.
- **Low Confidence (<60%)**: Treated as "Speculative Patterns" and labeled as such.

## 3. Hallucination Mitigation
- **Grounded Prompting**: Gemini is instructed to only use the provided logs and lineage IDs.
- **Negative Constraints**: The model is forbidden from recommending specific drug names, medical doses, or clinical diagnoses.
- **Explicit Context**: The system provides the model with its own "Confidence" heuristics to ensure the AI doesn't overstate its certainty.
